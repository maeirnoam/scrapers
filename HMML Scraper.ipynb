{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "from PIL import Image \n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen, HTTPSHandler\n",
    "import time\n",
    "import requests\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_link(link, ms_name, page, session, payload):\n",
    "    print('starting to scrape page {}...'.format(page))\n",
    "    tries = 3\n",
    "    i = 0\n",
    "    while i<3:\n",
    "        try:\n",
    "            s = session.get(link)\n",
    "#             soup = BeautifulSoup(open(html_path, 'r'),\"html.parser\",from_encoding=\"iso-8859-1\")\n",
    "            soup = BeautifulSoup(s.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "            path = 'C:\\\\Users\\\\User\\\\PycharmProjects\\\\scrapers\\\\{}\\\\{}.jpg'.format(ms_name,page)\n",
    "            if s.status_code == 200:\n",
    "                print('entered status 200')\n",
    "                s.raw.decode_content = True\n",
    "                with open(path,'wb') as f:\n",
    "                    for chunk in s:\n",
    "                        f.write(chunk)\n",
    "                break\n",
    "            else:\n",
    "                print('status problem: {}'.format(s.status_code))\n",
    "                i= i+1\n",
    "            \n",
    "        except:\n",
    "            print('status problem: {}'.format(s.status_code))\n",
    "            s = session.post(\"https://www.vhmml.org/j_spring_security_check\", data=payload)\n",
    "            print('retry..')\n",
    "            i= i+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vhmml format of images\n",
    "def scrape_vhmml_jpgs(ms_name, first_page, last_page, pattern):\n",
    "    \n",
    "    os.makedirs(ms_name, exist_ok=True)\n",
    "    session = requests.Session()\n",
    "    payload = {'j_username':'maeirnoam@gmail.com', \n",
    "              'j_password':'Kids1120'\n",
    "             }\n",
    "    s = session.post(\"https://www.vhmml.org/j_spring_security_check\", data=payload)\n",
    "    \n",
    "    if pattern=='single':\n",
    "        last = re.search(single_page_reg, last_page)\n",
    "        first = re.search(single_page_reg, first_page)\n",
    "        single_pages = [f\"{i:03d}\" for i in range(int(first.group(2)), int(last.group(2))+1)]\n",
    "        scrape_from_link(first_page, ms_name, f'{int(first.group(2))}', session, payload)\n",
    "        for i in range(len(single_pages)-2):\n",
    "            link = '{}{}{}'.format(first.group(1), single_pages[i+1],first.group(4))\n",
    "            scrape_from_link(link, ms_name, f'{int(first.group(2))+i+1}', session, payload)\n",
    "\n",
    "            \n",
    "#         if first.group(3)=='r':\n",
    "#             link = '{}{}{}{}'.format(first.group(1), first.group(2),\n",
    "#                                       'v', first.group(4))\n",
    "#             scrape_from_link(link, ms_name, f'{int(first.group(2))} v', session, payload)\n",
    "#         for i in range(len(single_pages)-2):\n",
    "#             link_r = '{}{}{}{}'.format(first.group(1), single_pages[i+1],\n",
    "#                                       'r', first.group(4))\n",
    "#             scrape_from_link(link_r, ms_name, f'{int(first.group(2))+i+1} r', session, payload)\n",
    "#             link_v = '{}{}{}{}'.format(first.group(1), single_pages[i+1],\n",
    "#                                       'v', first.group(4))\n",
    "#             scrape_from_link(link_v, ms_name, f'{int(first.group(2))+i+1} v', session, payload)\n",
    "#         if last.group(3)=='v':\n",
    "#             link = '{}{}{}{}'.format(first.group(1), last.group(2),\n",
    "#                                       'r', first.group(4))\n",
    "#             scrape_from_link(link, ms_name, f'{int(last.group(2))} r', session, payload)\n",
    "        \n",
    "        scrape_from_link(last_page, ms_name, f'{int(last.group(2))}', session, payload)\n",
    "    if pattern=='double':\n",
    "        last = re.search(double_page_reg, last_page)\n",
    "        first = re.search(double_page_reg, first_page)\n",
    "        double_pages = [f\"{i:03d}\" for i in range(int(first.group(4)), int(last.group(2))+1)]\n",
    "        first_page = first.group(4)\n",
    "        for i in range(len(double_pages)-1):\n",
    "            link = '{}_{}{}{}{}{}'.format(first.group(1), double_pages[i+1], first.group(3),\n",
    "                                         double_pages[i], first.group(5), first.group(6))\n",
    "            scrape_from_link(link, ms_name, f'{int(first.group(2))+i+1}', session, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to scrape page 11...\n",
      "entered status 200\n"
     ]
    }
   ],
   "source": [
    "ms_name= 'SMMJ_00180'\n",
    "last_page = 'https://www.vhmml.org/image/READING_ROOM/SMMJ/SMMJ%2000180/SMMJ_00180__102.JPG/full/1000,/0/default.jpg'\n",
    "first_page = 'https://www.vhmml.org/image/READING_ROOM/SMMJ/SMMJ%2000180/SMMJ_00180__011.JPG/full/1000,/0/default.jpg'\n",
    "first_part = 'https://www.vhmml.org/image/READING_ROOM/'\n",
    "second_part = 'SMMJ/SMMJ%2000180/SMMJ_00180'\n",
    "full_link = f'{first_part}{second_part}'\n",
    "\n",
    "# os.makedirs(ms_name, exist_ok=True)\n",
    "# session = requests.Session()\n",
    "# payload = {'j_username':'maeirnoam@gmail.com', \n",
    "#           'j_password':'Kids1120'\n",
    "#          }\n",
    "#s = session.post(\"https://www.vhmml.org/j_spring_security_check\", data=payload)\n",
    "\n",
    "#scrape_from_link(first_page, ms_name, 11, session, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pages = [f\"{i:03d}\" for i in range(int(first.group(2)), int(last.group(2))+1)]\n",
    "for i in range(len(single_pages)-2):\n",
    "    link = '{}{}{}{}'.format(first.group(1), single_pages[i+1],first.group(4))\n",
    "    scrape_from_link(link_r, ms_name, f'{int(first.group(2))+i+1}', session, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_name= 'SMMJ_00180'\n",
    "last_page = 'https://www.vhmml.org/image/READING_ROOM/SMMJ/SMMJ%2000180/SMMJ_00180__102.JPG/full/1000,/0/default.jpg'\n",
    "first_page = 'https://www.vhmml.org/image/READING_ROOM/SMMJ/SMMJ%2000180/SMMJ_00180__011.JPG/full/1000,/0/default.jpg'\n",
    "first_part = 'https://www.vhmml.org/image/READING_ROOM/'\n",
    "second_part = 'SMMJ/SMMJ%2000180/SMMJ_00180'\n",
    "full_link = f'{first_part}{second_part}'\n",
    "\n",
    "double_page_reg = re.compile('(.*)_(\\d+?)(r_)(\\d+?)(v\\..+)(.jpg)',\n",
    "            re.DOTALL | re.MULTILINE | re.UNICODE)\n",
    "single_page_reg = re.compile(\n",
    "        r'(https://www.vhmml.org/image/READING_ROOM/SMMJ/SMMJ%2000180/SMMJ_00180__)(\\d+)([rv]?)(\\..+jpg)',\n",
    "        re.DOTALL | re.MULTILINE | re.UNICODE)\n",
    "\n",
    "#last_page = 'https://www.vhmml.org/image/READING_ROOM/CET/CET%2000076/CET_00076_354v.JPG/full/1000,/0/default.jpg'\n",
    "#first_page = 'https://www.vhmml.org/image/READING_ROOM/CET/CET%2000076/CET_00076_092r.JPG/full/1000,/0/default.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to scrape page 11...\n",
      "entered status 200\n",
      "starting to scrape page 12...\n",
      "entered status 200\n",
      "starting to scrape page 13...\n",
      "entered status 200\n",
      "starting to scrape page 14...\n",
      "entered status 200\n",
      "starting to scrape page 15...\n",
      "entered status 200\n",
      "starting to scrape page 16...\n",
      "entered status 200\n",
      "starting to scrape page 17...\n",
      "entered status 200\n",
      "starting to scrape page 18...\n",
      "entered status 200\n",
      "starting to scrape page 19...\n",
      "entered status 200\n",
      "starting to scrape page 20...\n",
      "entered status 200\n",
      "starting to scrape page 21...\n",
      "entered status 200\n",
      "starting to scrape page 22...\n",
      "entered status 200\n",
      "starting to scrape page 23...\n",
      "entered status 200\n",
      "starting to scrape page 24...\n",
      "entered status 200\n",
      "starting to scrape page 25...\n",
      "entered status 200\n",
      "starting to scrape page 26...\n",
      "entered status 200\n",
      "starting to scrape page 27...\n",
      "entered status 200\n",
      "starting to scrape page 28...\n",
      "entered status 200\n",
      "starting to scrape page 29...\n",
      "entered status 200\n",
      "starting to scrape page 30...\n",
      "entered status 200\n",
      "starting to scrape page 31...\n",
      "entered status 200\n",
      "starting to scrape page 32...\n",
      "entered status 200\n",
      "starting to scrape page 33...\n",
      "entered status 200\n",
      "starting to scrape page 34...\n",
      "entered status 200\n",
      "starting to scrape page 35...\n",
      "entered status 200\n",
      "starting to scrape page 36...\n",
      "entered status 200\n",
      "starting to scrape page 37...\n",
      "entered status 200\n",
      "starting to scrape page 38...\n",
      "entered status 200\n",
      "starting to scrape page 39...\n",
      "entered status 200\n",
      "starting to scrape page 40...\n",
      "entered status 200\n",
      "starting to scrape page 41...\n",
      "entered status 200\n",
      "starting to scrape page 42...\n",
      "entered status 200\n",
      "starting to scrape page 43...\n",
      "entered status 200\n",
      "starting to scrape page 44...\n",
      "entered status 200\n",
      "starting to scrape page 45...\n",
      "entered status 200\n",
      "starting to scrape page 46...\n",
      "entered status 200\n",
      "starting to scrape page 47...\n",
      "entered status 200\n",
      "starting to scrape page 48...\n",
      "entered status 200\n",
      "starting to scrape page 49...\n",
      "entered status 200\n",
      "starting to scrape page 50...\n",
      "entered status 200\n",
      "starting to scrape page 51...\n",
      "entered status 200\n",
      "starting to scrape page 52...\n",
      "entered status 200\n",
      "starting to scrape page 53...\n",
      "entered status 200\n",
      "starting to scrape page 54...\n",
      "entered status 200\n",
      "starting to scrape page 55...\n",
      "entered status 200\n",
      "starting to scrape page 56...\n",
      "entered status 200\n",
      "starting to scrape page 57...\n",
      "entered status 200\n",
      "starting to scrape page 58...\n",
      "entered status 200\n",
      "starting to scrape page 59...\n",
      "entered status 200\n",
      "starting to scrape page 60...\n",
      "entered status 200\n",
      "starting to scrape page 61...\n",
      "entered status 200\n",
      "starting to scrape page 62...\n",
      "entered status 200\n",
      "starting to scrape page 63...\n",
      "entered status 200\n",
      "starting to scrape page 64...\n",
      "entered status 200\n",
      "starting to scrape page 65...\n",
      "entered status 200\n",
      "starting to scrape page 66...\n",
      "entered status 200\n",
      "starting to scrape page 67...\n",
      "entered status 200\n",
      "starting to scrape page 68...\n",
      "entered status 200\n",
      "starting to scrape page 69...\n",
      "entered status 200\n",
      "starting to scrape page 70...\n",
      "entered status 200\n",
      "starting to scrape page 71...\n",
      "entered status 200\n",
      "starting to scrape page 72...\n",
      "entered status 200\n",
      "starting to scrape page 73...\n",
      "entered status 200\n",
      "starting to scrape page 74...\n",
      "entered status 200\n",
      "starting to scrape page 75...\n",
      "entered status 200\n",
      "starting to scrape page 76...\n",
      "entered status 200\n",
      "starting to scrape page 77...\n",
      "entered status 200\n",
      "starting to scrape page 78...\n",
      "entered status 200\n",
      "starting to scrape page 79...\n",
      "entered status 200\n",
      "starting to scrape page 80...\n",
      "entered status 200\n",
      "starting to scrape page 81...\n",
      "entered status 200\n",
      "starting to scrape page 82...\n",
      "entered status 200\n",
      "starting to scrape page 83...\n",
      "entered status 200\n",
      "starting to scrape page 84...\n",
      "entered status 200\n",
      "starting to scrape page 85...\n",
      "entered status 200\n",
      "starting to scrape page 86...\n",
      "entered status 200\n",
      "starting to scrape page 87...\n",
      "entered status 200\n",
      "starting to scrape page 88...\n",
      "entered status 200\n",
      "starting to scrape page 89...\n",
      "entered status 200\n",
      "starting to scrape page 90...\n",
      "entered status 200\n",
      "starting to scrape page 91...\n",
      "entered status 200\n",
      "starting to scrape page 92...\n",
      "entered status 200\n",
      "starting to scrape page 93...\n",
      "entered status 200\n",
      "starting to scrape page 94...\n",
      "entered status 200\n",
      "starting to scrape page 95...\n",
      "entered status 200\n",
      "starting to scrape page 96...\n",
      "entered status 200\n",
      "starting to scrape page 97...\n",
      "entered status 200\n",
      "starting to scrape page 98...\n",
      "entered status 200\n",
      "starting to scrape page 99...\n",
      "entered status 200\n",
      "starting to scrape page 100...\n",
      "entered status 200\n",
      "starting to scrape page 101...\n",
      "entered status 200\n",
      "starting to scrape page 102...\n",
      "entered status 200\n"
     ]
    }
   ],
   "source": [
    "#this is working, run from here\n",
    "scrape_vhmml_jpgs(ms_name, first_page, last_page, pattern='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 'https://iiif.sinaimanuscripts.library.ucla.edu/iiif/2/ark%3A%2F21198%2Fz1wd5j69%2F5x91tv66/full/1000,/0/default.jpg'\n",
    "manifest = 'https://iiif.library.ucla.edu/ark%3A%2F21198%2Fz1wd5j69/manifest'\n",
    "# get all id's according to '@type\":\"dctypes:Image'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
